<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>zh&sol;00006&lowbar;peft&period;md</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

/* From extension ms-toolsai.jupyter */
/* These classnames are inherited from bootstrap, but are present in most notebook renderers */

.alert {
    width: auto;
    padding: 1em;
    margin-top: 1em;
    margin-bottom: 1em;
}
.alert > *:last-child {
    margin-bottom: 0;
}
#preview > .alert:last-child {
    /* Prevent this being set to zero by the default notebook stylesheet */
    padding-bottom: 1em;
}

.alert-success {
    /* Note there is no suitable color available, so we just copy "info" */
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-info {
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-warning {
    background-color: var(--theme-warning-background);
    color: var(--theme-warning-foreground);
}
.alert-danger {
    background-color: var(--theme-error-background);
    color: var(--theme-error-foreground);
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <!--
# zh/00006_peft.md
# 
# git pull from huggingface/transformers by LuYF-Lemon-love <luyanfeng_nlp@qq.com> on Apr 4, 2024
# updated by LuYF-Lemon-love <luyanfeng_nlp@qq.com> on Apr 4, 2024
# 
# ğŸ¤— PEFTï¼šåœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
-->
<h2 id="-peftåœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ">ğŸ¤— PEFTï¼šåœ¨ä½èµ„æºç¡¬ä»¶ä¸Šå¯¹åäº¿è§„æ¨¡æ¨¡å‹è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ</h2>
<p align="center">
    <img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00006_peft/thumbnail.png" width="500" />
</p>
<h2 id="åŠ¨æœº">åŠ¨æœº</h2>
<p>åŸºäº Transformers æ¶æ„çš„å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ï¼Œå¦‚ GPTã€T5 å’Œ BERTï¼Œå·²ç»åœ¨å„ç§è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä»»åŠ¡ä¸­å–å¾—äº†æœ€å…ˆè¿›çš„ç»“æœã€‚æ­¤å¤–ï¼Œè¿˜å¼€å§‹æ¶‰è¶³å…¶ä»–é¢†åŸŸï¼Œä¾‹å¦‚è®¡ç®—æœºè§†è§‰ (CV) (VITã€Stable Diffusionã€LayoutLM) å’ŒéŸ³é¢‘ (Whisperã€XLS-R)ã€‚ä¼ ç»Ÿçš„èŒƒå¼æ˜¯å¯¹é€šç”¨ç½‘ç»œè§„æ¨¡æ•°æ®è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç„¶åå¯¹ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚ä¸ä½¿ç”¨å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒ LLM (ä¾‹å¦‚ï¼Œé›¶æ ·æœ¬æ¨ç†) ç›¸æ¯”ï¼Œåœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šå¾®è°ƒè¿™äº›é¢„è®­ç»ƒ LLM ä¼šå¸¦æ¥å·¨å¤§çš„æ€§èƒ½æå‡ã€‚</p>
<p>ç„¶è€Œï¼Œ<strong>éšç€æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œåœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå…¨éƒ¨å‚æ•°çš„å¾®è°ƒå˜å¾—ä¸å¯è¡Œ</strong>ã€‚æ­¤å¤–ï¼Œ<strong>ä¸ºæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡ç‹¬ç«‹å­˜å‚¨å’Œéƒ¨ç½²å¾®è°ƒæ¨¡å‹å˜å¾—éå¸¸æ˜‚è´µï¼Œå› ä¸ºå¾®è°ƒæ¨¡å‹ä¸åŸå§‹é¢„è®­ç»ƒæ¨¡å‹çš„å¤§å°ç›¸åŒ</strong>ã€‚å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT) æ–¹æ³•æ—¨åœ¨è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼</p>
<p><strong>PEFT æ–¹æ³•ä»…å¾®è°ƒå°‘é‡ (é¢å¤–) æ¨¡å‹å‚æ•°ï¼ŒåŒæ—¶å†»ç»“é¢„è®­ç»ƒ LLM çš„å¤§éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œå¤§å¤§é™ä½äº†è®¡ç®—å’Œå­˜å‚¨æˆæœ¬</strong>ã€‚<strong>è¿™ä¹Ÿå…‹æœäº†<a href="https://arxiv.org/abs/1312.6211">ç¾éš¾æ€§é—å¿˜</a>çš„é—®é¢˜ï¼Œè¿™æ˜¯åœ¨ LLM çš„å…¨å‚æ•°å¾®è°ƒæœŸé—´è§‚å¯Ÿåˆ°çš„ä¸€ç§ç°è±¡</strong>ã€‚<strong>PEFT æ–¹æ³•ä¹Ÿæ˜¾ç¤ºå‡ºåœ¨ä½æ•°æ®çŠ¶æ€ä¸‹æ¯”å¾®è°ƒæ›´å¥½ï¼Œå¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°åŸŸå¤–åœºæ™¯</strong>ã€‚å®ƒå¯ä»¥åº”ç”¨äºå„ç§æ¨¡æ€ï¼Œä¾‹å¦‚<a href="https://github.com/huggingface/peft/tree/main/examples/image_classification">å›¾åƒåˆ†ç±»</a>ä»¥åŠ <a href="https://github.com/huggingface/peft/tree/main/examples/lora_dreambooth">Stable diffusion dreambooth</a>ã€‚</p>
<p><strong>PEFT æ–¹æ³•è¿˜æœ‰åŠ©äºæé«˜è½»ä¾¿æ€§ï¼Œå…¶ä¸­ç”¨æˆ·å¯ä»¥ä½¿ç”¨ PEFT æ–¹æ³•è°ƒæ•´æ¨¡å‹ï¼Œä»¥è·å¾—ä¸å®Œå…¨å¾®è°ƒçš„å¤§å‹æ£€æŸ¥ç‚¹ç›¸æ¯”ï¼Œå¤§å°ä»…å‡  MB çš„å¾®å°æ£€æŸ¥ç‚¹</strong>ã€‚ä¾‹å¦‚ï¼Œ <code>bigscience/mt0-xxl</code> å ç”¨ 40GB çš„å­˜å‚¨ç©ºé—´ï¼Œå…¨å‚æ•°å¾®è°ƒå°†å¯¼è‡´æ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†æœ‰å¯¹åº” 40GB æ£€æŸ¥ç‚¹ã€‚è€Œä½¿ç”¨ PEFT æ–¹æ³•ï¼Œæ¯ä¸ªä¸‹æ¸¸æ•°æ®é›†åªå ç”¨å‡  MB çš„å­˜å‚¨ç©ºé—´ï¼ŒåŒæ—¶å®ç°ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚<strong>æ¥è‡ª PEFT æ–¹æ³•çš„å°‘é‡è®­ç»ƒæƒé‡è¢«æ·»åŠ åˆ°é¢„è®­ç»ƒ LLM é¡¶å±‚ã€‚å› æ­¤ï¼ŒåŒä¸€ä¸ª LLM å¯ä»¥é€šè¿‡æ·»åŠ å°çš„æƒé‡æ¥ç”¨äºå¤šä¸ªä»»åŠ¡ï¼Œè€Œæ— éœ€æ›¿æ¢æ•´ä¸ªæ¨¡å‹ã€‚</strong></p>
<p><strong>ç®€è€Œè¨€ä¹‹ï¼ŒPEFT æ–¹æ³•ä½¿æ‚¨èƒ½å¤Ÿè·å¾—ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ï¼ŒåŒæ—¶åªæœ‰å°‘é‡å¯è®­ç»ƒå‚æ•°ã€‚</strong></p>
<p>ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´åœ°ä»‹ç» <a href="https://github.com/huggingface/peft">ğŸ¤— PEFT</a> åº“ã€‚å®ƒæä¾›äº†æœ€æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯ï¼Œä¸ ğŸ¤— Transformers å’Œ ğŸ¤— Accelerate æ— ç¼é›†æˆã€‚è¿™ä½¿å¾—èƒ½å¤Ÿä½¿ç”¨æ¥è‡ª Transformers çš„æœ€æµè¡Œå’Œé«˜æ€§èƒ½çš„æ¨¡å‹ï¼Œä»¥åŠ Accelerate çš„ç®€å•æ€§å’Œå¯æ‰©å±•æ€§ã€‚ä»¥ä¸‹æ˜¯ç›®å‰æ”¯æŒçš„ PEFT æ–¹æ³•ï¼Œå³å°†æ¨å‡ºæ›´å¤š:</p>
<ol>
<li>LoRA: <a href="https://arxiv.org/pdf/2106.09685.pdf">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li>
<li>Prefix Tuning: <a href="https://arxiv.org/pdf/2110.07602.pdf">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a></li>
<li>Prompt Tuning: <a href="https://arxiv.org/pdf/2104.08691.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li>P-Tuning: <a href="https://arxiv.org/pdf/2103.10385.pdf">GPT Understands, Too</a></li>
</ol>
<h2 id="ç”¨ä¾‹">ç”¨ä¾‹</h2>
<p>æˆ‘ä»¬åœ¨ GitHub PEFT åº“ä¸­æ¢ç´¢äº†è®¸å¤šæœ‰è¶£çš„<a href="https://github.com/huggingface/peft#use-cases">ç”¨ä¾‹</a>ã€‚ä»¥ä¸‹ç½—åˆ—çš„æ˜¯å…¶ä¸­æœ€æœ‰è¶£çš„:</p>
<ol>
<li>
<p>ä½¿ç”¨ ğŸ¤— PEFT LoRA åœ¨å…·æœ‰ 11GB RAM çš„æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè°ƒæ•´ <code>bigscience/T0_3B</code> æ¨¡å‹ (30 äº¿ä¸ªå‚æ•°)ï¼Œä¾‹å¦‚ Nvidia GeForce RTX 2080 Tiã€Nvidia GeForce RTX 3080 ç­‰ï¼Œå¹¶ä¸”ä½¿ç”¨ ğŸ¤— Accelerate çš„ DeepSpeed é›†æˆ: <a href="https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py">peft_lora_seq2seq_accelerate_ds_zero3_offload.py</a>ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨ Google Colab ä¸­è°ƒæ•´å¦‚æ­¤å¤§çš„ LLMã€‚</p>
</li>
<li>
<p>é€šè¿‡ä½¿ç”¨ ğŸ¤— PEFT LoRA å’Œ <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> åœ¨ Google Colab ä¸­å¯ç”¨ OPT-6.7b æ¨¡å‹ (67 äº¿ä¸ªå‚æ•°) çš„ INT8 è°ƒæ•´ï¼Œå°†å‰é¢çš„ç¤ºä¾‹æå‡ä¸€ä¸ªæ¡£æ¬¡: <a href="https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>ã€‚</p>
</li>
<li>
<p>åœ¨å…·æœ‰ 11GB RAM çš„æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šä½¿ç”¨ ğŸ¤— PEFT è¿›è¡Œç¨³å®šçš„ Diffusion Dreambooth è®­ç»ƒï¼Œä¾‹å¦‚ Nvidia GeForce RTX 2080 Tiã€Nvidia GeForce RTX 3080 ç­‰ã€‚è¯•ç”¨ Space æ¼”ç¤ºï¼Œå®ƒåº”è¯¥å¯ä»¥åœ¨ T4 å®ä¾‹ (16GB GPU) ä¸Šæ— ç¼è¿è¡Œ: <a href="https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth">smangrul/peft-lora-sd-dreambooth</a>ã€‚</p>
</li>
</ol>
<p align="center">
    <img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00006_peft/peft_lora_dreambooth_gradio_space.png" alt="peft lora dreambooth gradio space"><br>
    <em>PEFT LoRA Dreambooth Gradio Space</em>
</p>
<h2 id="ä½¿ç”¨--peft-è®­ç»ƒæ‚¨çš„æ¨¡å‹">ä½¿ç”¨ ğŸ¤— PEFT è®­ç»ƒæ‚¨çš„æ¨¡å‹</h2>
<p>è®©æˆ‘ä»¬è€ƒè™‘ä½¿ç”¨ LoRA å¾®è°ƒ <a href="https://huggingface.co/bigscience/mt0-large"><code>bigscience/mt0-large</code></a> çš„æƒ…å†µã€‚</p>
<ol>
<li><strong>å¼•è¿›å¿…è¦çš„åº“</strong></li>
</ol>
<pre><code class="language-diff">  from transformers import AutoModelForSeq2SeqLM
<span class="hljs-addition">+ from peft import get_peft_model, LoraConfig, TaskType</span>
  model_name_or_path = &quot;bigscience/mt0-large&quot;
  tokenizer_name_or_path = &quot;bigscience/mt0-large&quot;
</code></pre>
<ol start="2">
<li><strong>åˆ›å»º PEFT æ–¹æ³•å¯¹åº”çš„é…ç½®</strong></li>
</ol>
<pre><code class="language-py">peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=<span class="hljs-literal">False</span>, r=<span class="hljs-number">8</span>, lora_alpha=<span class="hljs-number">32</span>, lora_dropout=<span class="hljs-number">0.1</span>
)
</code></pre>
<ol start="3">
<li><strong>é€šè¿‡è°ƒç”¨ <code>get_peft_model</code> åŒ…è£…åŸºç¡€ ğŸ¤— Transformer æ¨¡å‹</strong></li>
</ol>
<pre><code class="language-diff">  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
<span class="hljs-addition">+ model = get_peft_model(model, peft_config)</span>
<span class="hljs-addition">+ model.print_trainable_parameters()</span>
# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282
</code></pre>
<p>å°±æ˜¯è¿™æ ·ï¼è®­ç»ƒå¾ªç¯çš„å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜ã€‚æœ‰å…³ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œè¯·å‚é˜…ç¤ºä¾‹ <a href="https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb">peft_lora_seq2seq.ipynb</a>ã€‚</p>
<ol start="4">
<li><strong>å½“æ‚¨å‡†å¤‡å¥½ä¿å­˜æ¨¡å‹ä»¥ä¾›æ¨ç†æ—¶ï¼Œåªéœ€æ‰§è¡Œä»¥ä¸‹æ“ä½œã€‚</strong></li>
</ol>
<pre><code class="language-py">model.save_pretrained(<span class="hljs-string">&quot;output_dir&quot;</span>) 
<span class="hljs-comment"># model.push_to_hub(&quot;my_awesome_peft_model&quot;) also works</span>
</code></pre>
<p><strong>è¿™åªä¼šä¿å­˜ç»è¿‡è®­ç»ƒçš„å¢é‡ PEFT æƒé‡ã€‚</strong> ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åœ¨æ­¤å¤„çš„ <code>twitter_complaints</code> raft æ•°æ®é›†ä¸Šæ‰¾åˆ°ä½¿ç”¨ LoRA è°ƒæ•´çš„ <code>bigscience/T0_3B</code>: <a href="https://huggingface.co/smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM">smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM</a>ã€‚è¯·æ³¨æ„ï¼Œå®ƒåªåŒ…å« 2 ä¸ªæ–‡ä»¶: <strong>adapter_config.json</strong> å’Œ <strong>adapter_model.bin</strong>ï¼Œåè€…åªæœ‰ 19MBã€‚</p>
<ol start="5">
<li><strong>è¦åŠ è½½å®ƒè¿›è¡Œæ¨ç†ï¼Œè¯·éµå¾ªä»¥ä¸‹ä»£ç ç‰‡æ®µ:</strong></li>
</ol>
<pre><code class="language-diff">  from transformers import AutoModelForSeq2SeqLM
<span class="hljs-addition">+ from peft import PeftModel, PeftConfig</span>

  peft_model_id = &quot;smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM&quot;
  config = PeftConfig.from_pretrained(peft_model_id)
  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
<span class="hljs-addition">+ model = PeftModel.from_pretrained(model, peft_model_id)</span>
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  model = model.to(device)
  model.eval()
  inputs = tokenizer(&quot;Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :&quot;, return_tensors=&quot;pt&quot;)

  with torch.no_grad():
      outputs = model.generate(input_ids=inputs[&quot;input_ids&quot;].to(&quot;cuda&quot;), max_new_tokens=10)
      print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
# &#x27;complaint&#x27;
</code></pre>
<h2 id="ä¸‹ä¸€æ­¥">ä¸‹ä¸€æ­¥</h2>
<p><strong>æˆ‘ä»¬å‘å¸ƒäº† PEFT æ–¹æ³•ï¼Œä½œä¸ºåœ¨ä¸‹æ¸¸ä»»åŠ¡å’ŒåŸŸä¸Šè°ƒæ•´å¤§å‹ LLM çš„æœ‰æ•ˆæ–¹å¼ï¼ŒèŠ‚çœäº†å¤§é‡è®¡ç®—å’Œå­˜å‚¨ï¼ŒåŒæ—¶å®ç°ä¸å…¨å‚æ•°å¾®è°ƒç›¸å½“çš„æ€§èƒ½ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªæœˆä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ›´å¤š PEFT æ–¹æ³•ï¼Œä¾‹å¦‚ (IA)3 å’Œç“¶é¢ˆé€‚é…å™¨ã€‚</strong> æ­¤å¤–ï¼Œæˆ‘ä»¬å°†å…³æ³¨æ–°çš„ç”¨ä¾‹ï¼Œä¾‹å¦‚ Google Colab ä¸­<a href="https://huggingface.co/openai/whisper-large"><code>whisper-large</code></a> æ¨¡å‹çš„ INT8 è®­ç»ƒä»¥åŠä½¿ç”¨ PEFT æ–¹æ³•è°ƒæ•´ RLHF ç»„ä»¶ (ä¾‹å¦‚ç­–ç•¥å’Œæ’åºå™¨)ã€‚</p>
<p>ä¸æ­¤åŒæ—¶ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°è¡Œä¸šä»ä¸šè€…å¦‚ä½•å°† PEFT åº”ç”¨äºä»–ä»¬çš„ç”¨ä¾‹ - å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–åé¦ˆï¼Œè¯·åœ¨æˆ‘ä»¬çš„ <a href="https://github.com/huggingface/peft">GitHub ä»“åº“</a> ä¸Šæå‡ºé—®é¢˜ ğŸ¤—ã€‚</p>
<p>ç¥ä½ æœ‰ä¸€è¶Ÿå¿«ä¹çš„å‚æ•°é«˜æ•ˆå¾®è°ƒä¹‹æ—…ï¼</p>

        
        
    </body>
    </html>