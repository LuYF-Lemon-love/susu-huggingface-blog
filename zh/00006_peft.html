<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>zh&sol;00006&lowbar;peft&period;md</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

/* From extension ms-toolsai.jupyter */
/* These classnames are inherited from bootstrap, but are present in most notebook renderers */

.alert {
    width: auto;
    padding: 1em;
    margin-top: 1em;
    margin-bottom: 1em;
}
.alert > *:last-child {
    margin-bottom: 0;
}
#preview > .alert:last-child {
    /* Prevent this being set to zero by the default notebook stylesheet */
    padding-bottom: 1em;
}

.alert-success {
    /* Note there is no suitable color available, so we just copy "info" */
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-info {
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-warning {
    background-color: var(--theme-warning-background);
    color: var(--theme-warning-foreground);
}
.alert-danger {
    background-color: var(--theme-error-background);
    color: var(--theme-error-foreground);
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <!--
# zh/00006_peft.md
# 
# git pull from huggingface/transformers by LuYF-Lemon-love <luyanfeng_nlp@qq.com> on Apr 4, 2024
# updated by LuYF-Lemon-love <luyanfeng_nlp@qq.com> on Apr 4, 2024
# 
# 🤗 PEFT：在低资源硬件上对十亿规模模型进行参数高效微调。
-->
<h2 id="-peft在低资源硬件上对十亿规模模型进行参数高效微调">🤗 PEFT：在低资源硬件上对十亿规模模型进行参数高效微调</h2>
<p align="center">
    <img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00006_peft/thumbnail.png" width="500" />
</p>
<h2 id="动机">动机</h2>
<p>基于 Transformers 架构的大型语言模型 (LLM)，如 GPT、T5 和 BERT，已经在各种自然语言处理 (NLP) 任务中取得了最先进的结果。此外，还开始涉足其他领域，例如计算机视觉 (CV) (VIT、Stable Diffusion、LayoutLM) 和音频 (Whisper、XLS-R)。传统的范式是对通用网络规模数据进行大规模预训练，然后对下游任务进行微调。与使用开箱即用的预训练 LLM (例如，零样本推理) 相比，在下游数据集上微调这些预训练 LLM 会带来巨大的性能提升。</p>
<p>然而，<strong>随着模型变得越来越大，在消费级硬件上对模型进行全部参数的微调变得不可行</strong>。此外，<strong>为每个下游任务独立存储和部署微调模型变得非常昂贵，因为微调模型与原始预训练模型的大小相同</strong>。参数高效微调(PEFT) 方法旨在解决这两个问题！</p>
<p><strong>PEFT 方法仅微调少量 (额外) 模型参数，同时冻结预训练 LLM 的大部分参数，从而大大降低了计算和存储成本</strong>。<strong>这也克服了<a href="https://arxiv.org/abs/1312.6211">灾难性遗忘</a>的问题，这是在 LLM 的全参数微调期间观察到的一种现象</strong>。<strong>PEFT 方法也显示出在低数据状态下比微调更好，可以更好地泛化到域外场景</strong>。它可以应用于各种模态，例如<a href="https://github.com/huggingface/peft/tree/main/examples/image_classification">图像分类</a>以及 <a href="https://github.com/huggingface/peft/tree/main/examples/lora_dreambooth">Stable diffusion dreambooth</a>。</p>
<p><strong>PEFT 方法还有助于提高轻便性，其中用户可以使用 PEFT 方法调整模型，以获得与完全微调的大型检查点相比，大小仅几 MB 的微小检查点</strong>。例如， <code>bigscience/mt0-xxl</code> 占用 40GB 的存储空间，全参数微调将导致每个下游数据集有对应 40GB 检查点。而使用 PEFT 方法，每个下游数据集只占用几 MB 的存储空间，同时实现与全参数微调相当的性能。<strong>来自 PEFT 方法的少量训练权重被添加到预训练 LLM 顶层。因此，同一个 LLM 可以通过添加小的权重来用于多个任务，而无需替换整个模型。</strong></p>
<p><strong>简而言之，PEFT 方法使您能够获得与全参数微调相当的性能，同时只有少量可训练参数。</strong></p>
<p>今天，我们很高兴地介绍 <a href="https://github.com/huggingface/peft">🤗 PEFT</a> 库。它提供了最新的参数高效微调技术，与 🤗 Transformers 和 🤗 Accelerate 无缝集成。这使得能够使用来自 Transformers 的最流行和高性能的模型，以及 Accelerate 的简单性和可扩展性。以下是目前支持的 PEFT 方法，即将推出更多:</p>
<ol>
<li>LoRA: <a href="https://arxiv.org/pdf/2106.09685.pdf">LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li>
<li>Prefix Tuning: <a href="https://arxiv.org/pdf/2110.07602.pdf">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a></li>
<li>Prompt Tuning: <a href="https://arxiv.org/pdf/2104.08691.pdf">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li>P-Tuning: <a href="https://arxiv.org/pdf/2103.10385.pdf">GPT Understands, Too</a></li>
</ol>
<h2 id="用例">用例</h2>
<p>我们在 GitHub PEFT 库中探索了许多有趣的<a href="https://github.com/huggingface/peft#use-cases">用例</a>。以下罗列的是其中最有趣的:</p>
<ol>
<li>
<p>使用 🤗 PEFT LoRA 在具有 11GB RAM 的消费级硬件上调整 <code>bigscience/T0_3B</code> 模型 (30 亿个参数)，例如 Nvidia GeForce RTX 2080 Ti、Nvidia GeForce RTX 3080 等，并且使用 🤗 Accelerate 的 DeepSpeed 集成: <a href="https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py">peft_lora_seq2seq_accelerate_ds_zero3_offload.py</a>。这意味着您可以在 Google Colab 中调整如此大的 LLM。</p>
</li>
<li>
<p>通过使用 🤗 PEFT LoRA 和 <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> 在 Google Colab 中启用 OPT-6.7b 模型 (67 亿个参数) 的 INT8 调整，将前面的示例提升一个档次: <a href="https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>。</p>
</li>
<li>
<p>在具有 11GB RAM 的消费级硬件上使用 🤗 PEFT 进行稳定的 Diffusion Dreambooth 训练，例如 Nvidia GeForce RTX 2080 Ti、Nvidia GeForce RTX 3080 等。试用 Space 演示，它应该可以在 T4 实例 (16GB GPU) 上无缝运行: <a href="https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth">smangrul/peft-lora-sd-dreambooth</a>。</p>
</li>
</ol>
<p align="center">
    <img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00006_peft/peft_lora_dreambooth_gradio_space.png" alt="peft lora dreambooth gradio space"><br>
    <em>PEFT LoRA Dreambooth Gradio Space</em>
</p>
<h2 id="使用--peft-训练您的模型">使用 🤗 PEFT 训练您的模型</h2>
<p>让我们考虑使用 LoRA 微调 <a href="https://huggingface.co/bigscience/mt0-large"><code>bigscience/mt0-large</code></a> 的情况。</p>
<ol>
<li><strong>引进必要的库</strong></li>
</ol>
<pre><code class="language-diff">  from transformers import AutoModelForSeq2SeqLM
<span class="hljs-addition">+ from peft import get_peft_model, LoraConfig, TaskType</span>
  model_name_or_path = &quot;bigscience/mt0-large&quot;
  tokenizer_name_or_path = &quot;bigscience/mt0-large&quot;
</code></pre>
<ol start="2">
<li><strong>创建 PEFT 方法对应的配置</strong></li>
</ol>
<pre><code class="language-py">peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=<span class="hljs-literal">False</span>, r=<span class="hljs-number">8</span>, lora_alpha=<span class="hljs-number">32</span>, lora_dropout=<span class="hljs-number">0.1</span>
)
</code></pre>
<ol start="3">
<li><strong>通过调用 <code>get_peft_model</code> 包装基础 🤗 Transformer 模型</strong></li>
</ol>
<pre><code class="language-diff">  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
<span class="hljs-addition">+ model = get_peft_model(model, peft_config)</span>
<span class="hljs-addition">+ model.print_trainable_parameters()</span>
# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282
</code></pre>
<p>就是这样！训练循环的其余部分保持不变。有关端到端示例，请参阅示例 <a href="https://github.com/huggingface/peft/blob/main/examples/conditional_generation/peft_lora_seq2seq.ipynb">peft_lora_seq2seq.ipynb</a>。</p>
<ol start="4">
<li><strong>当您准备好保存模型以供推理时，只需执行以下操作。</strong></li>
</ol>
<pre><code class="language-py">model.save_pretrained(<span class="hljs-string">&quot;output_dir&quot;</span>) 
<span class="hljs-comment"># model.push_to_hub(&quot;my_awesome_peft_model&quot;) also works</span>
</code></pre>
<p><strong>这只会保存经过训练的增量 PEFT 权重。</strong> 例如，您可以在此处的 <code>twitter_complaints</code> raft 数据集上找到使用 LoRA 调整的 <code>bigscience/T0_3B</code>: <a href="https://huggingface.co/smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM">smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM</a>。请注意，它只包含 2 个文件: <strong>adapter_config.json</strong> 和 <strong>adapter_model.bin</strong>，后者只有 19MB。</p>
<ol start="5">
<li><strong>要加载它进行推理，请遵循以下代码片段:</strong></li>
</ol>
<pre><code class="language-diff">  from transformers import AutoModelForSeq2SeqLM
<span class="hljs-addition">+ from peft import PeftModel, PeftConfig</span>

  peft_model_id = &quot;smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM&quot;
  config = PeftConfig.from_pretrained(peft_model_id)
  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)
<span class="hljs-addition">+ model = PeftModel.from_pretrained(model, peft_model_id)</span>
  tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

  model = model.to(device)
  model.eval()
  inputs = tokenizer(&quot;Tweet text : @HondaCustSvc Your customer service has been horrible during the recall process. I will never purchase a Honda again. Label :&quot;, return_tensors=&quot;pt&quot;)

  with torch.no_grad():
      outputs = model.generate(input_ids=inputs[&quot;input_ids&quot;].to(&quot;cuda&quot;), max_new_tokens=10)
      print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])
# &#x27;complaint&#x27;
</code></pre>
<h2 id="下一步">下一步</h2>
<p><strong>我们发布了 PEFT 方法，作为在下游任务和域上调整大型 LLM 的有效方式，节省了大量计算和存储，同时实现与全参数微调相当的性能。在接下来的几个月中，我们将探索更多 PEFT 方法，例如 (IA)3 和瓶颈适配器。</strong> 此外，我们将关注新的用例，例如 Google Colab 中<a href="https://huggingface.co/openai/whisper-large"><code>whisper-large</code></a> 模型的 INT8 训练以及使用 PEFT 方法调整 RLHF 组件 (例如策略和排序器)。</p>
<p>与此同时，我们很高兴看到行业从业者如何将 PEFT 应用于他们的用例 - 如果您有任何问题或反馈，请在我们的 <a href="https://github.com/huggingface/peft">GitHub 仓库</a> 上提出问题 🤗。</p>
<p>祝你有一趟快乐的参数高效微调之旅！</p>

        
        
    </body>
    </html>