<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>zh&sol;00004&lowbar;stackllama&period;md</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

/* From extension ms-toolsai.jupyter */
/* These classnames are inherited from bootstrap, but are present in most notebook renderers */

.alert {
    width: auto;
    padding: 1em;
    margin-top: 1em;
    margin-bottom: 1em;
}
.alert > *:last-child {
    margin-bottom: 0;
}
#preview > .alert:last-child {
    /* Prevent this being set to zero by the default notebook stylesheet */
    padding-bottom: 1em;
}

.alert-success {
    /* Note there is no suitable color available, so we just copy "info" */
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-info {
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-warning {
    background-color: var(--theme-warning-background);
    color: var(--theme-warning-foreground);
}
.alert-danger {
    background-color: var(--theme-error-background);
    color: var(--theme-error-foreground);
}

</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <!--
# zh/00004_stackllama.md
# 
# git pull from huggingface/transformers by LuYF-Lemon-love <luyanfeng_nlp@qq.com> on Apr 3, 2024
# updated by LuYF-Lemon-love <luyanfeng_nlp@qq.com> on Apr 3, 2024
# 
# “StackLLaMA”: 用 RLHF 训练 LLaMA 的手把手教程。
-->
<h1 id="stackllama-用-rlhf-训练-llama-的手把手教程">“StackLLaMA”: 用 RLHF 训练 LLaMA 的手把手教程</h1>
<p align="center">
    <img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/thumbnail.png" width="500" />
</p>
<p>如 <a href="https://openai.com/blog/chatgpt">ChatGPT</a>，<a href="https://openai.com/research/gpt-4">GPT-4</a>，<a href="https://www.anthropic.com/index/introducing-claude">Claude</a>语言模型 之强大，因为它们采用了 <strong>基于人类反馈的强化学习</strong> (Reinforcement Learning from Human Feedback, RLHF) 来使之更符合我们的使用场景。</p>
<p>本博客旨在展示用 RLHF 训练一个 <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai">LLaMA</a> 模型，以回答 <a href="https://stackexchange.com/">Stack Exchange</a> 上的问题。具体而言，包含以下几个方面:</p>
<ul>
<li><strong>有监督的微调 (Supervised Fine-tuning，SFT)。</strong></li>
<li><strong>奖励 / 偏好建模 (Reward / preference modeling，RM)。</strong></li>
<li><strong>基于人类反馈的强化学习 (RLHF)。</strong></li>
</ul>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/instructGPT.png" alt=""></p>
<p>摘自 InstructGPT 论文，Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” arXiv preprint arXiv:2203.02155 (2022).</p>
<p>结合了上述方法，我们发布了 StackLLaMA 模型，该模型在 <a href="https://huggingface.co/trl-lib/llama-se-rl-peft">🤗 Hub</a> 上开源 (访问链接查看 <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/">Meta 的原始 LLaMA</a> )，整个 <a href="https://huggingface.co/docs/trl/index">训练的流程</a> 已经集成到了 Hugging Face TRL 库中 。你可以通过下面的 <a href="https://huggingface.co/spaces/trl-lib/stack-llama">demo</a> 来尝试该模型。</p>
<h2 id="llama-模型">LLaMA 模型</h2>
<p>在实践 RLHF 时，选取一个合适的模型很重要: RLHF 只是一个让模型满足我们交互形式的需求的微调过程 。所以我们选取了最近上线的 <a href="https://arxiv.org/abs/2302.13971">LLaMA</a> 模型。LLaMA 模型是 Meta AI 最近推出的大语言模型。其参数量大小涵盖 7B 到 65B，以及训练在 1T 和 1.4T 的 token 上，这让其很实用。我们这里采用 7B 的模型。(请填写 Meta AI 的这份 <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">表单</a> 来下载模型)。</p>
<h2 id="stack-exchange-数据集">Stack Exchange 数据集</h2>
<p>收集人类的反馈数据集是很复杂且昂贵的劳动。为了做到这个，并且还能保证模型的有效性，我们使用 <a href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences">StackExchange 数据集</a>。该数据集涵盖了 StackExchange 平台上的问题和答案 (包含 StackOverflow 的编程等话题下的)。这很适合我们的实践，因为其包含了每个答案的赞和踩的数量。</p>
<p>我们按照 <a href="https://arxiv.org/abs/2112.00861">Askell et al. 2021</a> 中的方法，给每个答案赋分:</p>
<pre><code>score = log2 (1 + upvotes) rounded to the nearest integer, plus 1 if the questioner accepted the answer (we assign a score of −1 if the number of upvotes is negative).
</code></pre>
<p>对奖励模型，我们将看到每个问题总是需要两个答案对比。有些问题有很多答案，可以产生很多对，我们只取十个以限制每个问题的数据量。最后，我们把格式从 HTML 转化到 Markdown 以提高输出的可读性。你可以看到数据集和处理过程的 [笔记本]。(<a href="https://huggingface.co/datasets/lvwerra/stack-exchange-paired%E3%80%82">https://huggingface.co/datasets/lvwerra/stack-exchange-paired。</a>)</p>
<h2 id="高效训练策略">高效训练策略</h2>
<p>即使是最小 LLaMA 模型的训练，都需要大量内存。估算一下: 以 bf16 半精度，每个参数用 2 个字节 (以 fp32 精度四字节的标准)，训练时需要 8 个字节 (例如 Adam 优化器，参见 Tramsformers 的 <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer">性能文档</a>)。可见 7B 参数量的模型将用 (2+8)* 7B = 70 GB 的内存，并且还可能需要更多用于计算诸如注意力分数的中间值。所以很难在一张 80GB 显存的 A100 上训练。或许你可以使用一些技巧，比如用更高效的半精度训练的优化器来压缩内存，但溢出是迟早的。</p>
<p>另外的可能是 <strong>参数高效的微调</strong>(Parameter-Efficient Fine-Tuning, PEFT) 技术，比如 <a href="https://github.com/huggingface/peft"><code>peft</code></a> 库，它可以对使用 8-bit 加载的模型做 <strong>低秩优化</strong>(Low-Rank Adaptation，LoRA)。</p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/lora-animated.gif" alt=""></p>
<p>线性层的低秩优化: <strong>额外参数 (橙色) 被加在 Frozen 层 (蓝色)，编码后的隐藏状态与 Frozen 层的隐藏状态叠加在一起。</strong></p>
<p><strong>以 8bit 加载模型会大幅降低内存占用，因为每个参数只要一字节 (比如 7B LLaMA 是 7GB 内存)。与直接训练原始模型不同，LoRA 在特定层 (一般是注意力层) 添加少量新参数，大幅降低了需要训练的参数。</strong></p>
<p><strong>此情此景，一个衡量标准是 1B 的参数在整个微调过程中占 ~1.2-1.4GB (和具体 batch size 及序列长度有关)。在参考的博客中具体讨论了，这使得低成本下微调较大参数规模的模型成为可能 (比如在一张 A100 上微调 50-60B 的参数)。</strong></p>
<p>这些技术能让微调大模型的任务，在消费级设备和 Google Colab 上执行。这里提供一些值得关注的演示 demo: <code>facebook/opt-6.7b</code> (在 float16 精度下 13GB) 和 <code>openai/whisper-large</code>
跑在 Google Colab (15GB 显存) 上。欲了解 <code>peft</code> 的使用，请参见 <a href="https://github.com/huggingface/peft">github 仓库</a> 或者之前的 <a href="https://huggingface.co/blog/trl-peft">博客介绍</a>: 在客户端训练 20B 参数量的模型。</p>
<p>现在我们能在一张 GPU 上微调很大的模型了，但训练还是会很慢。此时最简单的策略便是并行化: <strong>把一个训练同时放到不同的 GPU 上，各 GPU 接受不同的 batch。这样我们可以并行执行前向传播和后向传播，通过增加 GPU 的数量实现并行能力提升。</strong></p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/chapter10_ddp.png" alt=""></p>
<p>我们可以选用 <code>trainsformers.Trainer</code> 或 <code>accelerate</code>，因为它们都支持无代码变更进行数据并行化。只需注意调用 <code>torchrun</code> 或者 <code>accelerate launch</code> 脚本时的参数即可实现。比如以下就是在一个 8 显卡的机器上分别用 <code>accelerate launch</code> 和 <code>torchrun</code>的方法:</p>
<pre><code class="language-bash">accelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py
torchrun --nnodes 1  --nproc_per_node 8 my_torch_script.py
</code></pre>
<h2 id="有监督的微调">有监督的微调</h2>
<p>在训练奖励模型和用 RL 之前，模型若是已经在我们感兴趣的方面表现好将会很有帮助。在我们的示例中，我们想要其能回答问题，而其他时候，我们可能它能听指令 (这时对指令执行的微调是理想的)。实现这个最简单的方法便是面向该语言任务，用该任务和领域的文本，继续训练。<a href="https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences">StackExchange 数据集</a> 含 10M 的指令量，所以我们能用其子集很容易地训练。</p>
<p>在用 RLHF 之前的模型微调没有特别的，就是一般的面向语言任务的预训练模型微调。为了高效利用数据，我们采用了称之为 <strong>打包</strong> 的技术: <strong>我们没有让 batch 中的每个样本均由单一文本组成，最后基于最长的文本来 padding (填充)，而是把很多文本拼接起来，用 EOS token 来隔开，然后分割成一些 chunk (切块) 来做成 batch，避免 padding。</strong></p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/chapter10_preprocessing-clm.png" alt=""></p>
<p><strong>该方法大大提高了效率，因为模型输入的所有 token 都对 loss 有所训练，而非 padding 作为掩码被丢弃了。如果你没有足够数据，并且担心随意地分开 token 会失去上下文语义，你也可以用传统的数据加载器。</strong> <code>ConstantLengthDataset</code> 解决了 <strong>打包</strong>技术，并且我们能在用 <code>peft</code> 加载模型后用 <code>Trainer</code>。首先，我们用 <code>int8</code> 加载模型，准备训练，然后加入 <code>LoRA</code> 微调器。</p>
<pre><code class="language-python"><span class="hljs-comment"># load model in 8bit</span>
model = AutoModelForCausalLM.from_pretrained(
        args.model_path,
        load_in_8bit=<span class="hljs-literal">True</span>,
        device_map={<span class="hljs-string">&quot;&quot;</span>: Accelerator().local_process_index}
    )
model = prepare_model_for_int8_training(model)

<span class="hljs-comment"># add LoRA to model</span>
lora_config = LoraConfig(
    r=<span class="hljs-number">16</span>,
    lora_alpha=<span class="hljs-number">32</span>,
    lora_dropout=<span class="hljs-number">0.05</span>,
    bias=<span class="hljs-string">&quot;none&quot;</span>,
    task_type=<span class="hljs-string">&quot;CAUSAL_LM&quot;</span>,
)

model = get_peft_model(model, config)
</code></pre>
<p><strong>我们根据相应的语言任务，对模型训练几千个 step (步)，并保存模型。由于我们将会有其他微调模型的目的，我们将 LoRA 的微调器权重合并到原模型中。</strong></p>
<p><strong>声明</strong>: 因为 LLaMA 的许可证规定，我们只能发布微调器的权重，你需要填 Meta AI 的 <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform">表格</a> 来获取模型，然后用这个 <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py">脚本</a> 来转成 🤗 Transformers 格式。注意 🤗 Transformers 应该从源码安装，或者 <code>v4.28</code> 版。</p>
<p>现在我们已经微调好了模型，可以训练奖励模型了。</p>
<h2 id="奖励模型和人类偏好">奖励模型和人类偏好</h2>
<p>原则上，我们可以直接用人类标注来对模型做 RLHF 微调。然而，这将需要我们给人类发送一些样本，在每轮优化后计分。这是贵且慢的，因为收敛需要的训练样本量大，而人类阅读和标注的速度有限。</p>
<p>一个比直接反馈更好的策略是，在进入 RL 循环之前用人类标注集来训练一个奖励模型。奖励模型的目的是模拟人类对文本的打分。构建奖励模型有许多能用的策略: 最直接的便是预测标注 (比如根据好与坏，输出比分或者布尔值)。<strong>最佳实践是，预测结果的排序，即对每个 prompt (输入文本) 对应的两个结果 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_k, y_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，模型预测人类标注的比分哪个更高。</strong></p>
<p>或者表示为 loss (损失) 函数:</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mi>D</mi></mrow></msub><mo stretchy="false">[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>r</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>y</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">loss(\theta) = - E_{(x, y_j, y_k)~D} [ log( \sigma( r_\theta (x, y_j) - r_\theta(x, y_k)) ) ]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">oss</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1275em;vertical-align:-0.3775em;"></span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mspace nobreak mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3775em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)))]</span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 是模型的得分，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">y_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 是更好的候选回答。</p>
<p><strong>在 StackExchange 数据集上，我们能得到两个答案的受欢迎程度。有了这个信息和上面的损失函数，我们就能自定义 loss 来改 <code>transformers.Trainer</code> 了。</strong></p>
<pre><code class="language-python">
<span class="hljs-keyword">class</span> <span class="hljs-title class_">RewardTrainer</span>(<span class="hljs-title class_ inherited__">Trainer</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span></span>):
        rewards_j = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids_j&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask_j&quot;</span>])[<span class="hljs-number">0</span>]
        rewards_k = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids_k&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask_k&quot;</span>])[<span class="hljs-number">0</span>]
        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()
        <span class="hljs-keyword">if</span> return_outputs:
            <span class="hljs-keyword">return</span> loss, {<span class="hljs-string">&quot;rewards_j&quot;</span>: rewards_j, <span class="hljs-string">&quot;rewards_k&quot;</span>: rewards_k}
        <span class="hljs-keyword">return</span> loss
</code></pre>
<p>我们用数据集中的 100000 对，并在 50000 对上评估。在比较小的 batch size，为 4 下，我们用 LoRA 的  <code>peft</code> 微调器来训练 LLaMA 模型，在 BF16 精度下用 Adam 优化器。我们的 LoRA 设置是:</p>
<pre><code class="language-python">peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=<span class="hljs-literal">False</span>,
    r=<span class="hljs-number">8</span>,
    lora_alpha=<span class="hljs-number">32</span>,
    lora_dropout=<span class="hljs-number">0.1</span>,
)
</code></pre>
<p>训练用 <a href="https://wandb.ai/krasul/huggingface/runs/wmd8rvq6?workspace=user-krasul">Weights &amp; Biases</a> 来记日志，并在 🤗 训练集群上，用 8 卡 A-100，要数小时，最后准确率为 <strong>67%</strong> 。尽管看上去可能低了，但想想这个任务的难度。</p>
<p>如下文要细说的，训练结果将作为固定参数，以供下游使用。</p>
<h2 id="基于人类反馈的强化学习">基于人类反馈的强化学习</h2>
<p>现在我们手头有了微调的语言模型和奖励模型，可以开始执行 RL 循环了: 这个过程大致分为三步</p>
<ol>
<li><strong>生成对 prompt (输入文本) 的反馈。</strong></li>
<li><strong>用奖励模型来对反馈评分。</strong></li>
<li><strong>对评分，进行一轮策略优化的强化学习。</strong></li>
</ol>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/trl_loop.png" alt=""></p>
<p>在被 token 化并输入奖励模型前，提问和回答的 prompt 模版如下:</p>
<pre><code>Question: &lt;Query&gt;
Answer: &lt;Response&gt;
</code></pre>
<p>在有监督训练 (SFT)，奖励模型训练 (RM) 和 RLHF 的阶段都用此模版。</p>
<p>用 RL 训练语言模型出现的常见问题是，模型可能学会胡说八道以糊弄奖励模型，后者可能给高分。为了权衡，我们对奖励增加惩罚: 留一份没有训练的模型，如何比较两者输出的 KL 散度</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>−</mo><mi>β</mi><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">R(x, y) = r(x, y) - \beta KL(x,y)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span></span></span></span> 是奖励模型的结果，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">KL(x,y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 是当前模型和对比模型的 KL 散度差。</p>
<p>再提一遍，我们用 <code>peft</code> 来实现内存高效的训练，其对 RLHF 阶段提供了优势。<strong>这里参考的模型和训练的模型用同一个基底，也就是有监督训练 (SFT) 的结果，它是用 8-bit 来加载，并且自始自终是固定的。我们仅用 PPO 方法优化最终模型的 LoRA 权重，同时全部共享一个基底模型。</strong></p>
<pre><code class="language-python"><span class="hljs-keyword">for</span> epoch, batch <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">enumerate</span>(ppo_trainer.dataloader)):
    question_tensors = batch[<span class="hljs-string">&quot;input_ids&quot;</span>]
        
    <span class="hljs-comment"># sample from the policy and generate responses</span>
    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=<span class="hljs-literal">False</span>,
        length_sampler=output_length_sampler,
        **generation_kwargs,
    )
    batch[<span class="hljs-string">&quot;response&quot;</span>] = tokenizer.batch_decode(response_tensors, skip_special_tokens=<span class="hljs-literal">True</span>)

    <span class="hljs-comment"># Compute sentiment score</span>
    texts = [q + r <span class="hljs-keyword">for</span> q, r <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(batch[<span class="hljs-string">&quot;query&quot;</span>], batch[<span class="hljs-string">&quot;response&quot;</span>])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;score&quot;</span>] - script_args.reward_baseline) <span class="hljs-keyword">for</span> output <span class="hljs-keyword">in</span> pipe_outputs]

    <span class="hljs-comment"># Run PPO step</span>
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    <span class="hljs-comment"># Log stats to WandB</span>
    ppo_trainer.log_stats(stats, batch, rewards)
</code></pre>
<p>我们用 🤗 集群，在 3x8 A100-80GB 的机器上训练了 20h，但一个差不多的结果很快 (大概，在 8 A100-80GB 上训练 20h)。所有的训练过程都在 <a href="https://wandb.ai/lvwerra/trl/runs/ie2h4q8p">Weight &amp; Biases</a> 上找到。</p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/wandb_reward.png" alt=""></p>
<p>每个 batch 的奖励，对每步的训练，在  ~1000 步时模型的效果最好。</p>
<p>所以模型训好了能干啥嘞 ? 我们拭目以待 !</p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/llama_prompt.png" alt=""></p>
<p>尽管我们不该太相信其结果，至少目前。但结果已经很好了，甚至附上了 Google 链接。我们来看看训练时的挑战。</p>
<h2 id="挑战不稳定和突破口">挑战，不稳定和突破口</h2>
<p><strong>用 RL 训练 LLM (Large Language Models，大语言模型) 不总是一帆风顺的，你看到的本文也是经历无数实验，无数失败和无数调参的。即便如此，该模型也不能说变现完美。这儿，我们分享一些遇到的观察和问题。</strong></p>
<h3 id="奖励更高代表更好表现-">奖励更高代表更好表现 ?</h3>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/logs_high_reward.png" alt=""></p>
<p><strong>天呐，这个实验肯定表现很好 ! 看奖励的曲线多甜啊 !</strong></p>
<p>在 RL 中，一般而言，奖励越高越好。在 RLHF 中，我们用了一个奖励模型，它不完美，所以留给了 PPO 算法捡漏的机会。这能导致奖励突然上升，然而当检查文本结果时，却充斥了字符 “```”，因为奖励模型对含有代码 stack exchange 的答案更信任。幸运的是，该问题碰到的很少，应该是采取的 KL 散度的惩罚项起到了作用。</p>
<h3 id="kl-散度总是正的">KL 散度总是正的?</h3>
<p><strong>如我们前面所提到的，一个 KL 惩罚项被用来保证训练后的分布和原始分布接近。一般地 , KL 散度来度量两个分布的相似程度，并且总是正的。</strong> 然而，在 <code>trl</code> 我们用了一个 KL 的近似，期望值和真的 KL 散度相同。</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><msub><mi>L</mi><mrow><mi>p</mi><mi>e</mi><mi>n</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msubsup><mi>π</mi><mi>ϕ</mi><mrow><mi>R</mi><mi>L</mi></mrow></msubsup><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><msup><mi>π</mi><mrow><mi>S</mi><mi>F</mi><mi>T</mi></mrow></msup><mo stretchy="false">(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">KL_{pen} (x, y) = log(\pi_\phi^{RL}(y | x) / \pi^{SFT}(y|x))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2744em;vertical-align:-0.3831em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">ϕ</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord">/</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">SFT</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></p>
<p>显然，当训练中一个 token 比原始模型概率低，这会导致 KL 散度为负，但是平均而言，这将是正的，否则您将无法从策略中正确采样。但是，某些采样策略可能会迫使某些 token 生成，或者某些 token 受到抑制。例如，当以批处理生成时，完成的序列会被填充；在设置最小长度时，EOS token 被抑制。模型会有很大/很小的概率得到负 KL 散度的 token。同时 PPO 算法是面向奖励优化的，模型就会追逐负的惩罚，导致训练不稳定。</p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/logs_neg_kl.png" alt=""></p>
<p><strong>对生成和采样，你需要特别小心。我们建议在诉诸更复杂的生成方法之前始终先使用简单的采样策略。</strong></p>
<h3 id="任然存在的问题">任然存在的问题</h3>
<p>仍然有很多问题我们不懂，比如下面，loss 间断地跳跃，导致之后的不稳定。</p>
<p><img src="file:////home/luyanfeng/my_code/github/huggingface/susu-huggingface-blog/images/00004_stackllama/logs_loss_spikes.png" alt=""></p>
<p>一旦我们解决了这些问题，我们就会上传变化到 <code>trl</code> 上，以保证社区受益。</p>
<h2 id="总结">总结</h2>
<p>在本博客，我们走过了 RLHF 训练的整个流程，从准备人类标注的数据集开始，调整语言模型到特定领域，训练奖励模型，并最终用 RL 训练一个模型。</p>
<p>通过使用 <code>peft</code>，任何人都能在一张 GPU 上跑我们的实验 ! 如果训练慢了，可以用数据并行化的方法，不需要改任何代码，或者用多张 GPU 并行提高训练速度。</p>
<p>对实际应用，这仅仅是第一步 ! 一旦你有了模型，你就要和其他模型比较优劣。这个可以用一个面向不同模型的排名生成做到，和我们训练奖励数据集类似。</p>
<p>一旦你加入了评估的步骤，好玩的就开始了: 你可以在原数据集上反复炼丹，也可以增加数据集或者对原数据集提纯。另外，你可以对奖励模型和生成模型尝试不同大小和结构的模型，这需要时间。</p>
<p>我们在积极提高 TRL 以保证 RLHF 的每一步都可见，并且十分激动能看到人们用它来构建的东西。如果你想有所贡献，欢迎看我们的 <a href="https://github.com/lvwerra/trl/issues">Github Issue</a>。</p>

        <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </body>
    </html>